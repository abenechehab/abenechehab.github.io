<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Abdelhakim Benechehab </title> <meta name="author" content="Abdelhakim Benechehab"> <meta name="description" content="The list of my publications and preprints can be found below."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?cdb0359b850271378211bb0a2bf8eacb"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://abenechehab.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Abdelhakim</span> Benechehab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">code </a> </li> <li class="nav-item "> <a class="nav-link" href="/podcast/">podcast </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_benechehab.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">The list of my publications and preprints can be found below.</p> </header> <article> <div class="publications"> For a complete list, see my <a href="https://scholar.google.com/citations?user=JxgqOKwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"> Google Scholar</a>. <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ebb134"> <a href="https://sites.google.com/view/llm-eval-workshop" rel="external nofollow noopener" target="_blank">workshop @ NeurIPS</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/youssef.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/youssef.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="youssef.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hili2025incontext" class="col-sm-8"> <div class="title">In-Context Meta-Learning with Large Language Models for Automated Model and Hyperparameter Selection</div> <div class="author"> Youssef Attia El Hili, Albert Thomas, <em>Abdelhakim Benechehab</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Corentin Léger, Corinne Ancourt, Balàzs Kégl' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=Ppo2SETqcE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Model and hyperparameter selection is a critical yet costly step in machine learning, often requiring expert intuition or extensive search. We investigate whether large language models (LLMs) can reduce this cost by acting as in-context meta-learners that generalize across tasks to propose effective model-hyperparameter choices without iterative optimization. Each task is represented as structured metadata, and we prompt an LLM under two strategies: Zero-Shot, using only the target task metadata, and Meta-Informed, which augments the prompt with metadata–recommendation pairs from prior tasks. Evaluated on 22 tabular Kaggle challenges, Meta-Informed prompting outperforms Zero-Shot and hyperparameter optimization baselines, approaching expert AutoML blends while yielding interpretable reasoning traces and efficiency gains under tight training budgets. These results suggest that LLMs can transfer knowledge across tasks to guide automated model selection, establishing model and hyperparameter selection as a concrete testbed for studying emergent adaptation beyond language domains.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hili2025incontext</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{In-Context Meta-Learning with Large Language Models for Automated Model and Hyperparameter Selection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Attia El Hili, Youssef and Thomas, Albert and Benechehab, Abdelhakim and Léger, Corentin and Ancourt, Corinne and Kégl, Balàzs}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">day</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#D9004C"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/adapts.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/adapts.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="adapts.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="benechehab2025adapts" class="col-sm-8"> <div class="title">AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting</div> <div class="author"> <em>Abdelhakim Benechehab</em>, Vasilii Feofanov, Giuseppe Paolo, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Albert Thomas, Maurizio Filippone, Balázs Kégl' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Forty-second International Conference on Machine Learning (ICML)</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.10235" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2502.10235" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/abenechehab/AdaPTS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/adapts_final_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">benechehab2025adapts</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Benechehab, Abdelhakim and Feofanov, Vasilii and Paolo, Giuseppe and Thomas, Albert and Filippone, Maurizio and Kégl, Balázs}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Forty-second International Conference on Machine Learning (ICML)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">day</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#D9004C"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dicl.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/dicl.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dicl.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="benechehab2024zero" class="col-sm-8"> <div class="title">Zero-shot Model-based Reinforcement Learning using Large Language Models</div> <div class="author"> <em>Abdelhakim Benechehab</em>, Youssef Attia El Hili, Ambroise Odonnat, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Oussama Zekri, Albert Thomas, Giuseppe Paolo, Maurizio Filippone, Ievgen Redko, Balázs Kégl' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>The Thirteenth International Conference on Learning Representations (ICLR)</em>, Jan 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.11711" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=uZFXpPrwSh" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/abenechehab/dicl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/dicl_final_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/dicl.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://iclr.cc/virtual/2025/poster/27971" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. In this paper, we investigate how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes. We identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs’ deployment in this setup and propose Disentangled In-Context Learning (DICL) to address them. We present proof-of-concept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods. Our experiments further demonstrate that our approach produces well-calibrated uncertainty estimates.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">benechehab2024zero</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Zero-shot Model-based Reinforcement Learning using Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Benechehab, Abdelhakim and Hili, Youssef Attia El and Odonnat, Ambroise and Zekri, Oussama and Thomas, Albert and Paolo, Giuseppe and Filippone, Maurizio and Redko, Ievgen and K{\'e}gl, Bal{\'a}zs}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Thirteenth International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">day</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arxiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/llm_preview.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/llm_preview.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="llm_preview.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zekri2024large" class="col-sm-8"> <div class="title">Large Language Models as Markov Chains</div> <div class="author"> Oussama Zekri, Ambroise Odonnat, <em>Abdelhakim Benechehab</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Linus Bleistein, Nicolas Boullé, Ievgen Redko' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Preprint</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.02724" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2410.02724" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this challenging task by drawing an equivalence between generic autoregressive language models with vocabulary of size T and context window of size K and Markov chains defined on a finite state space of size O(T^K). We derive several surprising findings related to the existence of a stationary distribution of Markov chains that capture the inference power of LLMs, their speed of convergence to it, and the influence of the temperature on the latter. We then prove pre-training and in-context generalization bounds and show how the drawn equivalence allows us to enrich their interpretation. Finally, we illustrate our theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zekri2024large</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Large Language Models as Markov Chains}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zekri, Oussama and Odonnat, Ambroise and Benechehab, Abdelhakim and Bleistein, Linus and Boull{\'e}, Nicolas and Redko, Ievgen}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">day</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ebb134"> <a href="https://iclworkshop.github.io/" rel="external nofollow noopener" target="_blank">workshop @ ICML</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sgd.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/sgd.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sgd.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zekri2024can" class="col-sm-8"> <div class="title">Can LLMs predict the convergence of Stochastic Gradient Descent?</div> <div class="author"> Oussama Zekri, <em>Abdelhakim Benechehab</em>, and Ievgen Redko </div> <div class="periodical"> <em>ICML 2024 Workshop ICL</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2408.01736" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/ICML_ICL_Workshop_Poster__SGDICL_.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Large-language models are notoriously famous for their impressive performance across a wide range of tasks. One surprising example of such impressive performance is a recently identified capacity of LLMs to understand the governing principles of dynamical systems satisfying the Markovian property. In this paper, we seek to explore this direction further by studying the dynamics of stochastic gradient descent in convex and non-convex optimization. By leveraging the theoretical link between the SGD and Markov chains, we show a remarkable zero-shot performance of LLMs in predicting the local minima to which SGD converges for previously unseen starting points. On a more general level, we inquire about the possibility of using LLMs to perform zero-shot randomized trials for larger deep learning models used in practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zekri2024can</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Can LLMs predict the convergence of Stochastic Gradient Descent?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zekri, Oussama and Benechehab, Abdelhakim and Redko, Ievgen}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ICML 2024 Workshop ICL}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">day</span> <span class="p">=</span> <span class="s">{18}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ebb134"> <a href="https://sites.google.com/view/rlc2024-icbinb" rel="external nofollow noopener" target="_blank">workshop @ RLC</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/msloss.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/msloss.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="msloss.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="benechehab2024study" class="col-sm-8"> <div class="title">A Study of the Weighted Multi-step Loss Impact on the Predictive Error and the Return in MBRL</div> <div class="author"> <em>Abdelhakim Benechehab</em>, Albert Thomas, Giuseppe Paolo, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Maurizio Filippone, Balázs Kégl' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>RLC 2024 Workshop ICBINB</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=K4VjW7evSV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/WCclCOjy6vw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/assets/pdf/dlrlss_multi_step_poster_benechehab.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/Multi_step_loss_ICBINB_RLC24_Presentation.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>In model-based reinforcement learning, most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows. In this paper we tackle this issue by using a multi-step objective to train one-step models. Our objective is a weighted sum of the mean squared error (MSE) loss at various future horizons. We find that this new loss is particularly useful when the data is noisy (additive Gaussian noise in the observations), which is often the case in real-life environments. We show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged R2-score on future prediction horizons. To our surprise, in the pure batch reinforcement learning setting, we find that the multi-step loss-based models perform only marginally better than the baseline. Furthermore, this improvement is only observed for small loss horizons, unlike the trend present with the R2-score on the respective datasets.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">benechehab2024study</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Study of the Weighted Multi-step Loss Impact on the Predictive Error and the Return in MBRL}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Benechehab, Abdelhakim and Thomas, Albert and Paolo, Giuseppe and Filippone, Maurizio and K{\'e}gl, Bal{\'a}zs}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{RLC 2024 Workshop ICBINB}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">day</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://iclr-blogposts.github.io/2024/about/" rel="external nofollow noopener" target="_blank">blogpost @ ICLR</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/blog.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/blog.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="blog.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thomas2024fair" class="col-sm-8"> <div class="title">Fair Model-Based Reinforcement Learning Comparisons with Explicit and Consistent Update Frequency</div> <div class="author"> Albert Thomas, <em>Abdelhakim Benechehab</em>, Giuseppe Paolo, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Balázs Kégl' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>The Third Blogpost Track at ICLR 2024</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://iclr-blogposts.github.io/2024/blog/update-frequency-in-mbrl/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="/assets/pdf/ICLR_24_Blogpost_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Implicit update frequencies can introduce ambiguity in the interpretation of model-based reinforcement learning benchmarks, obscuring the real objective of the evaluation. While the update frequency can sometimes be optimized to improve performance, real-world applications often impose constraints, allowing updates only between deployments on the actual system. This blog post emphasizes the need for evaluations using consistent update frequencies across different algorithms to provide researchers and practitioners with clearer comparisons under realistic constraints.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">thomas2024fair</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fair Model-Based Reinforcement Learning Comparisons with Explicit and Consistent Update Frequency}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Thomas, Albert and Benechehab, Abdelhakim and Paolo, Giuseppe and K{\'e}gl, Bal{\'a}zs}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Third Blogpost Track at ICLR 2024}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">day</span> <span class="p">=</span> <span class="s">{16}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/msl_iclr.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/msl_iclr.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="msl_iclr.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="benechehab2023multi" class="col-sm-8"> <div class="title">Multi-timestep models for Model-based Reinforcement Learning</div> <div class="author"> <em>Abdelhakim Benechehab</em>, Giuseppe Paolo, Albert Thomas, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Maurizio Filippone, Balázs Kégl' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Preprint</em>, Feb 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.05672" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2310.05672" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In model-based reinforcement learning (MBRL), most algorithms rely on simulating trajectories from one-step dynamics models learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as length of the trajectory grows. In this paper we tackle this issue by using a multi-timestep objective to train one-step models. Our objective is a weighted sum of a loss function (e.g., negative log-likelihood) at various future horizons. We explore and test a range of weights profiles. We find that exponentially decaying weights lead to models that significantly improve the long-horizon R2 score. This improvement is particularly noticeable when the models were evaluated on noisy data. Finally, using a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, we found that our multi-timestep models outperform or match standard one-step models. This was especially evident in a noisy variant of the considered environment, highlighting the potential of our approach in real-world applications.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">benechehab2023multi</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-timestep models for Model-based Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Benechehab, Abdelhakim and Paolo, Giuseppe and Thomas, Albert and Filippone, Maurizio and K{\'e}gl, Bal{\'a}zs}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/offline.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/offline.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="offline.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="benechehab2022deep" class="col-sm-8"> <div class="title">Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning</div> <div class="author"> <em>Abdelhakim Benechehab</em>, Albert Thomas, and Balázs Kégl </div> <div class="periodical"> <em>Preprint</em>, Feb 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.02858" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2402.02858" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization. Following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts. This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system. The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain. We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark. We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">benechehab2022deep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Benechehab, Abdelhakim and Thomas, Albert and K{\'e}gl, Bal{\'a}zs}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Abdelhakim Benechehab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: October 09, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>